{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yw4509/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are using the old version of `pytorch_pretrained_bert`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from table_bert import TableBertModel\n",
    "from table_bert import Table, Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, minimum_count=1):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "\n",
    "        self.index2word = [None] * 4\n",
    "        self.index2word[SOS_IDX] = SOS_TOKEN\n",
    "        self.index2word[EOS_IDX] = EOS_TOKEN\n",
    "        self.index2word[UNK_IDX] = UNK_TOKEN\n",
    "        self.index2word[PAD_IDX] = PAD_TOKEN\n",
    "\n",
    "        self.word2count[SOS_TOKEN] = 100;\n",
    "        self.word2count[EOS_TOKEN] = 100;\n",
    "        self.word2count[UNK_TOKEN] = 100;\n",
    "        self.word2count[PAD_TOKEN] = 100;\n",
    "\n",
    "        self.word2index[SOS_TOKEN] = SOS_IDX;\n",
    "        self.word2index[EOS_TOKEN] = EOS_IDX;\n",
    "        self.word2index[UNK_TOKEN] = UNK_IDX;\n",
    "        self.word2index[PAD_TOKEN] = PAD_IDX;\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "        self.minimum_count = minimum_count;\n",
    "\n",
    "    def add_ans(self, ans):\n",
    "        for word in ans:\n",
    "            self.addWord(word.lower())\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2count.keys():\n",
    "            self.word2count[word] = 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        if self.word2count[word] >= self.minimum_count:\n",
    "            if word not in self.index2word:\n",
    "                word = str(word);\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word.append(word)\n",
    "                self.n_words += 1\n",
    "\n",
    "    def vec2txt(self, list_idx):\n",
    "        word_list = []\n",
    "        if type(list_idx) == list:\n",
    "            for i in list_idx:\n",
    "                if i not in [EOS_IDX, SOS_IDX, PAD_IDX]:\n",
    "                    word_list.append(self.index2word[i])\n",
    "        else:\n",
    "            for i in list_idx:\n",
    "                if i.item() not in [EOS_IDX, SOS_IDX, PAD_IDX]:\n",
    "                    word_list.append(self.index2word[i.item()])\n",
    "        return (' ').join(word_list)\n",
    "\n",
    "    def txt2vec(self, ans):\n",
    "        token_list = ans;\n",
    "        index_list = [self.word2index[token] if token in self.word2index else UNK_IDX for token in token_list]\n",
    "        return torch.from_numpy(np.array(index_list)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class voc():\n",
    "    def __init__(self, df, voc_location, minimum_count, max_num):\n",
    "        #df here is answers, list of list of tokens\n",
    "        self.df=df\n",
    "        self.minimum_count = minimum_count;\n",
    "        self.max_num = max_num;\n",
    "        self.voc_location = voc_location;\n",
    "        self.main_df, self.target_voc = self.load_or_create_voc()\n",
    "        #main df includes target_tokenized, target_indized, target_len\n",
    "        #target_voc is the Lang class with full vocab and can perform idx to token, token to idx, token to count opertations\n",
    "    def __len__(self):\n",
    "        return len(self.main_df) if self.max_num is None else self.max_num\n",
    "    def __getitem__(self, idx):\n",
    "        return_list = [self.main_df.iloc[idx]['target_indized'], self.main_df.iloc[idx]['target_len'] ]\n",
    "        return return_list\n",
    "    def load_or_create_voc(self):\n",
    "        if not os.path.exists(self.voc_location):\n",
    "            os.makedirs(self.voc_location)\n",
    "        full_file_path = os.path.join(self.voc_location, 'mincnt_maxnum' +\n",
    "                                      str(self.minimum_count) + '_' + \\\n",
    "                                      str(self.max_num)+'.p')\n",
    "        #if the address exits, we will load the dictionary from the full path,\n",
    "        #ow, we will create a new voc dictionary and pickle dump to full path\n",
    "        if os.path.isfile(full_file_path):\n",
    "            print('Load Pre-existing Voc Dictionary')\n",
    "            target_voc = pickle.load(open(full_file_path,'rb'))\n",
    "        else:\n",
    "            print('Create New Voc Dictionary')\n",
    "            target_voc = Lang(minimum_count = self.minimum_count);\n",
    "            for ans in self.df: # load ans into voc\n",
    "                target_voc.add_ans(ans)\n",
    "            pickle.dump(target_voc,open(full_file_path,'wb'))\n",
    "        # change token to idx based on dictionary\n",
    "        indices_data = []\n",
    "        for ans in self.df: # ans tokens to idx\n",
    "            index_list = [target_voc.word2index[token] if token in target_voc.word2index else UNK_IDX for token in ans]\n",
    "            if len(index_list)<=self.max_num:\n",
    "                index_list = index_list + [PAD_IDX]*(self.max_num-len(index_list))\n",
    "            else:\n",
    "                index_list = index_list[:self.max_num]\n",
    "            index_list.append(EOS_IDX) # add EOS token to the answer\n",
    "            indices_data.append(index_list)\n",
    "        main_df = pd.DataFrame();\n",
    "        main_df['target_tokenized'] = self.df;\n",
    "        main_df['target_indized'] = indices_data;\n",
    "        main_df['target_len'] = main_df['target_tokenized'].apply(lambda x: len(x)+1) #+1 for EOS\n",
    "        main_df =  main_df[main_df['target_len'] >=2] #filter out ans that are empty\n",
    "        return main_df,target_voc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset():\n",
    "    def __init__(self, path, voc_location, model, minimum_count, max_num):\n",
    "        # the initalization will end up with four parts: tabs, context, answers and target_voc\n",
    "        self.path = path\n",
    "        self.voc_location = voc_location\n",
    "        self.model = model\n",
    "        self.minimum_count=minimum_count\n",
    "        self.max_num=max_num\n",
    "\n",
    "        self.data = pd.read_json(self.path)\n",
    "        self.data['title'] = self.data['title'].fillna('unknown')\n",
    "        lens = self.data['answer'].apply(lambda x: len(model.tokenizer.tokenize(str(x[0]))))\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        # print(self.data.shape)\n",
    "\n",
    "        self.tabs = []\n",
    "        self.context = []\n",
    "        self.answers = []\n",
    "\n",
    "        self._build()\n",
    "        self.voc_obj = voc(self.answers, self.voc_location, minimum_count=minimum_count, max_num=max_num)\n",
    "\n",
    "        self.answers = self.voc_obj.main_df.target_indized.tolist()\n",
    "        self.target_voc=self.voc_obj.target_voc\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            qs = self.data.loc[idx, 'context']\n",
    "            ans = self.data.loc[idx, 'answer']\n",
    "            heads = self.data.loc[idx, 'header']\n",
    "            tit = self.data.loc[idx, 'title']\n",
    "            rs = self.data.loc[idx, 'rows']\n",
    "\n",
    "            col = [Column(z[0], z[1], sample_value=z[2]) for z in heads]\n",
    "            table = Table(\n",
    "                id=tit,\n",
    "                header=col,\n",
    "                data=rs\n",
    "            ).tokenize(self.model.tokenizer)\n",
    "            self.tabs.append(table)\n",
    "\n",
    "            self.context.append(self.model.tokenizer.tokenize(qs))\n",
    "            self.answers.append(self.model.tokenizer.tokenize(str(ans[0])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context)\n",
    "    def __getitem__(self, index):\n",
    "        tabi = self.tabs[index]\n",
    "        conti = self.context[index]\n",
    "        ansi = self.answers[index]\n",
    "        return {\"table\": tabi, \"context\": conti, \"answer\": ansi}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, voc_location, model, minimum_count=1,max_num=35):\n",
    "    return WikiDataset(path=path, voc_location=voc_location,model=model,\\\n",
    "                       minimum_count=minimum_count,max_num=max_num)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return [batch[i]['table'] for i in range(len(batch))], [batch[i]['context'] for i in range(len(batch))], torch.tensor([batch[i]['answer'] for i in range(len(batch))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Module(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        self.l1 = nn.Linear(hidden_dim, output_dim, bias=False)\n",
    "        self.l2 = nn.Linear(hidden_dim + output_dim, output_dim, bias=False)\n",
    "    def forward(self, hidden, encoder_outs):\n",
    "        ''' \n",
    "        hiddden: bsz x hidden_dim\n",
    "        encoder_outs: bsz x sq_len x encoder dim (output_dim)\n",
    "        src_lens: bsz\n",
    "\n",
    "        x: bsz x output_dim\n",
    "        attn_score: bsz x sq_le\n",
    "        '''\n",
    "        x = self.l1(hidden)\n",
    "        # [src_len,batch_size,enc_hid_dim]\n",
    "        encoder_outs = encoder_outs.permute(1,0,2) #[ batch_size,src_len,dim]\n",
    "        att_score = torch.bmm(encoder_outs, x.unsqueeze(-1)); #this is bsz x seq x 1\n",
    "        att_score = att_score.squeeze(-1); #this is bsz x seq\n",
    "        att_score = att_score.transpose(0, 1);\n",
    "        \n",
    "        batch_size = encoder_outs.shape[0]\n",
    "        src_len  = encoder_outs.shape[1]\n",
    "        src_lens = torch.ones(batch_size)*src_len\n",
    "        \n",
    "        seq_mask = self.sequence_mask(src_lens, \n",
    "                                    max_len=max(src_lens).item()).transpose(0, 1)\n",
    "        masked_att = seq_mask * att_score #[seq_len, batch_size], [seq_len, batch_size] element multiplication\n",
    "        masked_att[masked_att == 0] = -1e10\n",
    "        attn_scores = F.softmax(masked_att, dim=0) #[seq_len, batch_size]\n",
    "        # encoder_outs.transpose(0, 1) [seq_len, batch_size, dim] #[seq_len, batch_size,1]\n",
    "        x = (attn_scores.unsqueeze(2) * encoder_outs.transpose(0, 1)).sum(dim=0) #[batch_size, output_dim]\n",
    "        x = torch.tanh(self.l2(torch.cat((x, hidden), dim=1))) #[batch_size, output_dim]\n",
    "        return x, attn_scores\n",
    "\n",
    "    def sequence_mask(self, sequence_length, max_len=None, device = torch.device('cuda')):\n",
    "#         print('max len', max_len) 16\n",
    "#         print('sequence_length',sequence_length) [16,14,12,11,11,16...] shape [batch_size]\n",
    "        if max_len is None:\n",
    "            max_len = sequence_length.max().item()\n",
    "        batch_size = sequence_length.size(0) \n",
    "#         print('batch_size',batch_size)\n",
    "        seq_range = torch.arange(0, max_len).long()\n",
    "        seq_range_expand = seq_range.unsqueeze(0).repeat([batch_size, 1])\n",
    "        seq_range_expand = seq_range_expand #[batch_size, max_len]\n",
    "#         print('seq_range_expand shape',seq_range_expand.shape) \n",
    "        \n",
    "        seq_length_expand = (sequence_length.unsqueeze(1) # [batch_size,1]\n",
    "                             .expand_as(seq_range_expand)) # [batch_size,max_len] seq_length repeated for max_len times\n",
    "        # it returns a matrix of batch_size, max_len with diagonal above = True\n",
    "        return (seq_range_expand < seq_length_expand).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_SelfAttn(pl.LightningModule):\n",
    "    \"\"\"Generates a sequence of tokens in response to context with self attention.\n",
    "       Note that this is the same as previous decoder if self_attention=False\"\"\"\n",
    "    def __init__(self, output_size, hidden_size, idropout=0.5, self_attention = False, encoder_attention = False):\n",
    "        super(Decoder_SelfAttn, self).__init__()\n",
    "\n",
    "        self.output_size = output_size;\n",
    "\n",
    "        self.self_attention = self_attention;\n",
    "        self.encoder_attention = encoder_attention;\n",
    "\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size);\n",
    "\n",
    "        self.memory_rnn = nn.GRUCell(hidden_size + int(self.encoder_attention==True)*self.hidden_size, \n",
    "                                    hidden_size, bias=True);\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        if self.self_attention:\n",
    "            self.projector_summ = nn.Sequential(nn.Dropout(idropout),\n",
    "                                                nn.Linear(hidden_size*2, hidden_size),\n",
    "                                                nn.Dropout(idropout))\n",
    "        if self.encoder_attention:\n",
    "            self.encoder_attention_module = Attention_Module(self.hidden_size, self.hidden_size);\n",
    "        \n",
    "    def forward(self, input, memory, encoder_output = None, xs_len = None, context_vec = None):\n",
    "        # memory [12, 768]\n",
    "        memory = memory.unsqueeze(0)\n",
    "#         print('memory',memory.shape)\n",
    "        memory = memory.transpose(0, 1); #memory is decoder_hidden which is encoder_hidden here [64, 1, 512]\n",
    "#         print('memory',memory.shape)\n",
    "        emb = self.embedding(input) # input [64, 36] emb [64, 36, 512]\n",
    "        emb = F.relu(emb)\n",
    "        \n",
    "        emb = emb.transpose(0, 1); # emb [16,64,512]\n",
    "        return_scores = torch.empty(emb.size(0), emb.size(1), self.output_size) # [16, 64, 20111]\n",
    "        if context_vec is None and self.encoder_attention:\n",
    "            context_vec = torch.zeros([emb.size(1), self.hidden_size], device=self.device); #[64, 512]\n",
    "        if self.encoder_attention:\n",
    "            attn_wts_list = [];\n",
    "        else:\n",
    "            attn_wts_list = None;\n",
    "        for t in range(emb.size(0)): #range trg_len\n",
    "            current_vec = emb[t]; #[64, 512]\n",
    "            selected_memory = memory[:, 0, :]; #[64, 512]\n",
    "            if self.self_attention:\n",
    "#                 print('memory',memory.shape)\n",
    "                selected_memory, attention0 = self.calculate_self_attention(current_vec, memory)\n",
    "                # [64,521] [64, 1, 1]\n",
    "            if self.encoder_attention:\n",
    "                current_vec = torch.cat([current_vec, context_vec], dim = 1); #[64, 1024]\n",
    "            if ( not (self.self_attention or self.encoder_attention)):    \n",
    "                selected_memory, attention0 = memory[:, 0, :], None; ##[64, 512]\n",
    "            # recurrent\n",
    "            mem_out = self.memory_rnn(current_vec, selected_memory);#[64, 512] \n",
    "#             print('mem_out',mem_out.shape)\n",
    "            # GRUCell input x and hidden, output is the next hidden\n",
    "            if self.encoder_attention:\n",
    "                context_vec, attention0 = self.encoder_attention_module(mem_out, encoder_output);\n",
    "                scores = self.out(context_vec);\n",
    "                attn_wts_list.append(attention0)\n",
    "            else:\n",
    "                scores = self.out(mem_out) #linear hidden to output\n",
    "#                 print('scores',scores.shape)\n",
    "\n",
    "            scores = self.softmax(scores);\n",
    "            return_scores[t] = scores\n",
    "\n",
    "            if self.self_attention:\n",
    "                 # update memory\n",
    "                # [64, 1, 512], [64,0,512] => [64,1,512]\n",
    "                memory = torch.cat([mem_out[:, None, :], memory[:, :-1, :]], dim=1);\n",
    "            else:\n",
    "                memory = mem_out[:, None, :]; \n",
    "        return return_scores.transpose(0, 1).contiguous(), memory.transpose(0,1), attn_wts_list, context_vec\n",
    "\n",
    "    def calculate_self_attention(self, input, memory):\n",
    "#         print('input', input.shape) which is current_vector\n",
    "#         print('memory', memory.shape)\n",
    "        #input torch.Size([12, 768]) , memory torch.Size([1, 12, 768])\n",
    "        concat_vec = torch.cat([input,  memory[:, 0, :]], dim=1); #[64,1024]\n",
    "        projected_vec = self.projector_summ(concat_vec); #[64,512]\n",
    "        dot_product_values = torch.bmm(memory, projected_vec.unsqueeze(-1)).squeeze(-1)/ math.sqrt(self.hidden_size); #[64,1]\n",
    "        weights =  F.softmax(dot_product_values, dim = 1).unsqueeze(-1); #[64, 1, 1]\n",
    "        selected_memory = torch.sum( memory * weights, dim=1) #[64, 512]\n",
    "        return selected_memory, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaBERTTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams, decoder,enc_hid_dim, dec_hid_dim):\n",
    "        super(TaBERTTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.encoder = TableBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.hidden = nn.Linear(enc_hid_dim, dec_hid_dim)\n",
    "        self.decoder = decoder\n",
    "        self.START = torch.LongTensor([SOS_IDX]).to(device)\n",
    "\n",
    "    def forward(self, context_list, table_list, answer_list):\n",
    "        context_encoding, column_encoding, info_dict = self.encoder.encode(contexts=context_list, tables=table_list)\n",
    "        # print('context_encoding, column_encoding, answer_list shape:',context_encoding.shape,column_encoding.shape, answer_list.shape)\n",
    "        # context_encoding: [batch_size, token_size, 768] [12,17,768]\n",
    "        # column_encoding: [batch_size, number of columns, 768] [12,6,768]\n",
    "        # answer_list: [batch_size, 36] max_len is 35 and adds one for eos\n",
    "\n",
    "        src = torch.cat([context_encoding, column_encoding], dim=1)  # [batch_size,scr_len,768*2] [12,123,768]\n",
    "        encoder_output = src.permute(1,0,2)  # [src_len,batch_size,enc_hid_dim] [123,12,768]\n",
    "        batch_size = encoder_output.shape[1]\n",
    "        starts = self.START.expand(batch_size, 1)  #expand to batch size; start is [SOS]\n",
    "        # print('scr shape:', src.shape)\n",
    "        # print('encoder_outputs shape:', encoder_outputs.shape)\n",
    "        # print('batch size:',batch_size)\n",
    "\n",
    "        encoder_hidden = torch.tanh(self.hidden(torch.sum(encoder_output, axis=0)))  # [batch_size=12,dec_hid_dim=512]\n",
    "        # print('encoder hidden output shape', hidden.shape)\n",
    "\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        trg = answer_list # [batch_size,trg_len] [64,36]\n",
    "        trg_len = trg.shape[1]\n",
    "        # print('trg_len:',trg_len)\n",
    "        # print('trg shape:', trg.shape)\n",
    "\n",
    "        # cut off eos from the input\n",
    "        y_in = trg.narrow(1, 0, trg.size(1) - 1) #narrow(dim, start, length)\n",
    "        # add in sos in front of the sentence\n",
    "        decoder_input = torch.cat([starts, y_in], 1)\n",
    "        decoder_output, decoder_hidden, _, _ = self.decoder(decoder_input,\n",
    "                                                      encoder_hidden, \n",
    "                                                      encoder_output)\n",
    "        \n",
    "        return decoder_output\n",
    "\n",
    "    def _step(self, batch):\n",
    "        tbl, ctx, ans = batch[0], batch[1], torch.tensor(batch[2])\n",
    "        # print('step ----------------------------------------------------------------------------------------------------')\n",
    "        decoder_output = self(ctx, tbl, ans) # output = [trg len, batch size, output dim]\n",
    "        _max_score, predictions = decoder_output.max(2)\n",
    "        # ans = [trg len, batch size]   [w,w,w,w,eos]\n",
    "        # print('outputs from step:', outputs.shape)\n",
    "        # print('trg from step:', trg.shape)\n",
    "        output_dim = decoder_output.shape[-1]\n",
    "        \n",
    "        outputs = decoder_output.view(-1, output_dim) # output = [(trg len-0 ) * batch size, output dim]\n",
    "        trg = ans.contiguous().view(-1)  # trg = [(trg len - 0) * batch size] [w,w,w,w,eos]\n",
    "        # print('outputs from step:', outputs.shape)\n",
    "        # print('trg from step:', trg.shape)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "        outputs = outputs.to(device=device)\n",
    "        trg = trg.to(device=device)\n",
    "        loss = criterion(outputs, trg)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx,optimizer_idx):\n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "#         print('train epoch end--------------------------------------------------------')\n",
    "#         print(outputs)\n",
    "#         print(outputs[0])\n",
    "#         print(outputs[1])\n",
    "        avg_train_loss_encoder = torch.stack([x['loss'] for x in outputs[0]]).mean()\n",
    "        avg_train_loss_decoder = torch.stack([x['loss'] for x in outputs[1]]).mean()\n",
    "        avg_train_loss = torch.stack((avg_train_loss_encoder,avg_train_loss_decoder)).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, \"progress_bar\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch) \n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_val_loss}\n",
    "        return {\"avg_val_loss\": avg_val_loss, \"log\": tensorboard_logs, \"progress_bar\": tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters_encoder = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.encoder.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.encoder.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            }, ]\n",
    "#         optimizer_grouped_parameters_decoder = [\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in self.decoder.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": self.hparams.weight_decay,\n",
    "#             },\n",
    "#             {\n",
    "#                 \"params\": [p for n, p in self.decoder.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#                 \"weight_decay\": 0.0,\n",
    "#             }, ]\n",
    "        \n",
    "        encoder_optimizer = AdamW(optimizer_grouped_parameters_encoder, lr=self.hparams.lr,\n",
    "                                  eps=self.hparams.adam_epsilon)\n",
    "        decoder_optimizer = torch.optim.SGD(self.decoder.parameters(), lr=self.hparams.lr, nesterov=True,\n",
    "                                 momentum = self.hparams.momentum)\n",
    "#         decoder_optimizer = torch.optim.SGD(optimizer_grouped_parameters_decoder, lr=self.hparams.lr, nesterov=True,\n",
    "#                                  momentum = self.hparams.momentum)\n",
    "        optimizers = [encoder_optimizer,decoder_optimizer]\n",
    "        \n",
    "        schedulers = [\n",
    "            {'scheduler': ReduceLROnPlateau(encoder_optimizer, mode=\"min\", min_lr=7.5e-5, patience=5, verbose=True),\n",
    "                # might need to change here\n",
    "             'monitor': \"avg_train_loss\",  # Default: val_loss\n",
    "             'interval': 'epoch',\n",
    "             'frequency': 1\n",
    "                },\n",
    "            {'scheduler': ReduceLROnPlateau(decoder_optimizer, mode=\"min\", min_lr=7.5e-5, patience=5, verbose=True),\n",
    "                # might need to change here\n",
    "             'monitor': \"avg_train_loss\" ,  # Default: val_loss\n",
    "             'interval': 'epoch',\n",
    "             'frequency': 1\n",
    "                }\n",
    "        ]\n",
    "        return optimizers, schedulers\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(path=self.hparams.train_data, voc_location=self.hparams.voc_location,\n",
    "                                    model=self.encoder,minimum_count=self.hparams.minimum_count,\n",
    "                                    max_num=self.hparams.max_num)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size,\n",
    "                                drop_last=True, shuffle=True,num_workers=4, collate_fn=collate_fn)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(path=self.hparams.dev_data, voc_location=self.hparams.voc_location,\n",
    "                                  model=self.encoder)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4, \n",
    "                          collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "SEP_IDX = 4\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "SEP_TOKEN = '<sep>'  # separates utterances in the dialogue history\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger('tb_logs', name = 'my_run_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data': './data/train_tabert_0.01.json', 'dev_data': './data/dev_tabert_0.01.json', 'voc_location': './voc', 'output_dir': './check_point', 'minimum_count': 1, 'max_num': 35, 'lr': 0.00075, 'dampening': 0.9, 'momentum': 0.99, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'gradient_clip_val': 0.3, 'warmup_steps': 100, 'gradient_accumulation_steps': 16, 'train_batch_size': 32, 'eval_batch_size': 12, 'num_train_epochs': 2000, 'n_gpu': 1, 'fp_16': False, 'opt_level': 'O1', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "args_dict = dict(\n",
    "            train_data=\"./data/train_tabert_0.01.json\",\n",
    "            dev_data=\"./data/dev_tabert_0.01.json\",\n",
    "            voc_location='./voc',\n",
    "            output_dir=\"./check_point\",\n",
    "            minimum_count=1,\n",
    "            max_num=35,\n",
    "            lr=7.5e-4,\n",
    "            dampening=0.9, #increase 0.9-0.99. learning rate decrease by factor of 10\n",
    "            momentum=0.99,\n",
    "            weight_decay=1e-2,\n",
    "            adam_epsilon=1e-8,\n",
    "            gradient_clip_val = 0.3,\n",
    "            warmup_steps=100,\n",
    "            gradient_accumulation_steps=16,\n",
    "            train_batch_size=32,\n",
    "            eval_batch_size=12,\n",
    "            num_train_epochs=2000,\n",
    "            n_gpu=1,\n",
    "            fp_16=False, #fp_16 true will end up shorter trainning time. 32 is default\n",
    "            opt_level='O1', #pure or mixed precision\n",
    "            seed=42\n",
    "            # early_stop_callback=False,\n",
    "        )\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=args.output_dir, \n",
    "        prefix='checkpoint-{epoch:02d}',\n",
    "        monitor=\"val_loss\", mode=\"min\", save_top_k=5)\n",
    "\n",
    "train_params = dict(\n",
    "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "        gpus=args.n_gpu,\n",
    "        max_epochs=args.num_train_epochs,\n",
    "        amp_level=args.opt_level,\n",
    "        gradient_clip_val=args.gradient_clip_val,\n",
    "        auto_lr_find=True,\n",
    "        precision=32,\n",
    "        checkpoint_callback=checkpoint_callback\n",
    "        # early_stop_callback=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 11575\n",
    "\n",
    "ENC_HID_DIM = 768\n",
    "\n",
    "DEC_EMB_DIM = 516\n",
    "DEC_HID_DIM = 768\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "dec = Decoder_SelfAttn(output_size=OUTPUT_DIM, hidden_size=DEC_HID_DIM, idropout=0.5, \n",
    "                       self_attention = True, encoder_attention = True)\n",
    "model = TaBERTTuner(args,enc_hid_dim=ENC_HID_DIM,dec_hid_dim=DEC_HID_DIM,decoder=dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=True\n",
    "# if checkpoint:\n",
    "#     model = model.load_from_checkpoint('/scratch/yw4509/capstone/check_point/checkpoint-{epoch:02d}-epoch=825.ckpt',\n",
    "#                                   enc_hid_dim=ENC_HID_DIM,dec_hid_dim=DEC_HID_DIM,decoder=dec)\n",
    "#     model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.device.type != 'cuda':\n",
    "#         print('param {}, not on GPU'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "INFO:lightning:Set SLURM handle signals.\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | VanillaTableBert | 110 M \n",
      "1 | hidden  | Linear           | 590 K \n",
      "2 | decoder | Decoder_SelfAttn | 26 M  \n",
      "INFO:lightning:\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | VanillaTableBert | 110 M \n",
      "1 | hidden  | Linear           | 590 K \n",
      "2 | decoder | Decoder_SelfAttn | 26 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cda1afdfafe4961aafcb852fc0d2d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=84.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Pre-existing Voc Dictionary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138e374160714dbf86c183814838951c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Pre-existing Voc Dictionary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea357b2e24f4b02aa7338b6f3058b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 7.93 GiB total capacity; 7.18 GiB already allocated; 30.56 MiB free; 7.38 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x2af18de36536 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x2af18dbf6f1e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x2af18dbf7f9e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x2af158cc2fd5 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf9310b (0x2af1572bb10b in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfdc9f7 (0x2af1573049f7 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1075389 (0x2af148601389 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x10756c7 (0x2af1486016c7 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe2165e (0x2af1483ad65e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x9e0 (0x2af1483b3f50 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x1134321 (0x2af1486c0321 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x1187623 (0x2af148713623 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x28c7d12 (0x2af158befd12 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::Tensor at::native::(anonymous namespace)::host_softmax_backward<at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue, true>(at::Tensor const&, at::Tensor const&, long, bool) + 0xb9 (0x2af158c04729 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #14: at::native::log_softmax_backward_cuda(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x99 (0x2af158bf04a9 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: <unknown function> + 0xf9dd90 (0x2af1572c5d90 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x10c5ad6 (0x2af148651ad6 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2ca99fc (0x2af14a2359fc in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x10c5ad6 (0x2af148651ad6 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::generated::LogSoftmaxBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1c9 (0x2af149e31869 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x2d89c05 (0x2af14a315c05 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x2af14a312f03 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x2af14a313ce2 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_init(int) + 0x39 (0x2af14a30c359 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x2af1469a3378 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #25: <unknown function> + 0xb8408 (0x2af11c0ab408 in /share/apps/anaconda3/5.3.1/lib/libstdc++.so.6)\nframe #26: <unknown function> + 0x7e25 (0x2af111d1ce25 in /lib64/libpthread.so.0)\nframe #27: clone + 0x6d (0x2af11202fbad in /lib64/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-344b82eef671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_fit_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# train or test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;31m# TRAINING_STEP + TRAINING_STEP_END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;31m# when returning -1 from train_step, we end epoch early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    683\u001b[0m                     \u001b[0;31m# calculate loss (train step + train step end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0;31m# -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step_and_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                     batch_outputs = self._process_closure_result(\n\u001b[1;32m    687\u001b[0m                         \u001b[0mbatch_callback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_callback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, result, optimizer, opt_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             result.closure_loss = self.trainer.accelerator_backend.backward(\n\u001b[0;32m--> 827\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m             )\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, closure_loss, optimizer, opt_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# do backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# once backward has been applied, release graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \"\"\"\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_and_norm_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 7.93 GiB total capacity; 7.18 GiB already allocated; 30.56 MiB free; 7.38 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x2af18de36536 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x2af18dbf6f1e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x2af18dbf7f9e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x2af158cc2fd5 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf9310b (0x2af1572bb10b in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfdc9f7 (0x2af1573049f7 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1075389 (0x2af148601389 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x10756c7 (0x2af1486016c7 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe2165e (0x2af1483ad65e in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x9e0 (0x2af1483b3f50 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x1134321 (0x2af1486c0321 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x1187623 (0x2af148713623 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x28c7d12 (0x2af158befd12 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::Tensor at::native::(anonymous namespace)::host_softmax_backward<at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue, true>(at::Tensor const&, at::Tensor const&, long, bool) + 0xb9 (0x2af158c04729 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #14: at::native::log_softmax_backward_cuda(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x99 (0x2af158bf04a9 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: <unknown function> + 0xf9dd90 (0x2af1572c5d90 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x10c5ad6 (0x2af148651ad6 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2ca99fc (0x2af14a2359fc in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x10c5ad6 (0x2af148651ad6 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::generated::LogSoftmaxBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1c9 (0x2af149e31869 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x2d89c05 (0x2af14a315c05 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x2af14a312f03 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x2af14a313ce2 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_init(int) + 0x39 (0x2af14a30c359 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x2af1469a3378 in /home/yw4509/miniconda3/envs/tabert/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #25: <unknown function> + 0xb8408 (0x2af11c0ab408 in /share/apps/anaconda3/5.3.1/lib/libstdc++.so.6)\nframe #26: <unknown function> + 0x7e25 (0x2af111d1ce25 in /lib64/libpthread.so.0)\nframe #27: clone + 0x6d (0x2af11202fbad in /lib64/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "# torch.cuda.empty_cache()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run learning rate finder\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "\n",
    "# # Results can be found in\n",
    "# lr_finder.results\n",
    "\n",
    "# # Plot with\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# # Pick point based on plot, or get suggestion\n",
    "# new_lr = lr_finder.suggestion()\n",
    "\n",
    "# # update hparams of the model\n",
    "# model.hparams.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Parameters Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, child in model.named_children():\n",
    "    print('name: ', name)\n",
    "    print('isinstance({}, nn.Module): '.format(name), isinstance(child, nn.Module))\n",
    "    print('=====')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabert",
   "language": "python",
   "name": "tabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
